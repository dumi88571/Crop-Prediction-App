# -*- coding: utf-8 -*-
"""SENTIMENT_ANALYSIS_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h7DOwexUuD8fBikysUtvadvn_MU9ncr4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data=pd.read_csv(r'/content/sentiment.csv')
data

data.head(12)

data.value_counts()

data['Sentiment'].unique()

data.isnull().sum()

data.info()

data.describe()

data['Sentiment'] = data['Sentiment'].str.lower()

print(data)

import re

def clean_text(text):
    text = text.lower()
    # Removing stock symbols like $ESI
    text = re.sub(r'\$\w+', '', text)
    # Removing URLs
    text = re.sub(r'http\S+', '', text)
    # Removing punctuation/numbers
    text = re.sub(r'[^a-z\s]', '', text)
    # Removing extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

#  cleaning
data['cleaned_text'] = data['Sentence'].apply(clean_text)

print(data[['Sentence', 'cleaned_text', 'Sentiment']].head())

import re
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\$\w+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text
data['cleaned_text'] = data['Sentence'].apply(clean_text)
print(data[['Sentence', 'cleaned_text', 'Sentiment']].head())

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Downloading the VADER lexicon
nltk.download('vader_lexicon')

analyzer = SentimentIntensityAnalyzer()
def get_sentiment_score(text):
    score = analyzer.polarity_scores(text)
    return score['compound']
data['SentimentScore'] = data['cleaned_text'].apply(get_sentiment_score)

# Displaying the first few rows with the new column
print(data[['cleaned_text', 'Sentiment', 'SentimentScore']].tail(12))

from scipy.stats import skew

skewness_value = skew(data['SentimentScore'])
print(f'Skewness of sentiment scores: {skewness_value}')

skewness_value = -0.0395 # Using the value from the previous execution

if -0.5 <= skewness_value <= 0.5:
    skewness_interpretation = "approximately symmetrical"
    skewness_implication = "The sentiment scores are relatively evenly distributed around the mean."
elif skewness_value > 0.5:
    skewness_interpretation = "skewed to the right (positively skewed)"
    skewness_implication = "There are more data points with lower sentiment scores, and the tail of the distribution is longer on the positive side."
else: # skewness_value < -0.5
    skewness_interpretation = "skewed to the left (negatively skewed)"
    skewness_implication = "There are more data points with higher sentiment scores, and the tail of the distribution is longer on the negative side."

print(f'Skewness value: {skewness_value}')
print(f'Interpretation: The distribution of sentiment scores is {skewness_interpretation}.')
print(f'Implication: {skewness_implication}')

import matplotlib.pyplot as plt
import seaborn as sns

# Plot sentiment distribution
plt.figure(figsize=(6,4))
sns.countplot(x='Sentiment', data=data)
plt.title('Distribution of Sentiment Labels')
plt.show()

# Optional: Word clouds for positive and negative sentiment

from wordcloud import WordCloud

def plot_wordcloud(sentiment):
    subset = data[data['Sentiment'] == sentiment]
    text = ' '.join(subset['cleaned_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'WordCloud for {sentiment} Sentiment')
    plt.show()

plot_wordcloud('positive')
plot_wordcloud('negative')
plot_wordcloud('neutral')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

X = data['cleaned_text']
y = data['Sentiment']

# Converting text to TF-IDF features
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))
X_vec = vectorizer.fit_transform(X)

# Split into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_vec, y, test_size=0.2, random_state=42, stratify=y)
from sklearn.linear_model import LogisticRegression

# Instantiate and train model
model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, mean_absolute_error

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy Score: {accuracy:.4f}')
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_test_encoded = le.fit_transform(y_test)
y_pred_encoded = le.transform(y_pred)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test_encoded, y_pred_encoded)
print(f'Mean Squared Error (MSE): {mse:.4f}')

# Calculate R-squared (R2) score
r2 = r2_score(y_test_encoded, y_pred_encoded)
print(f'RÂ² Score: {r2:.4f}')

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test_encoded, y_pred_encoded)
print(f'Mean Absolute Error (MAE): {mae:.4f}')

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

#  y_pred contains my predicted sentiment labels (e.g., 'positive', 'negative', 'neutral')
pred_counts = pd.Series(y_pred).value_counts()

# Plot pie chart
plt.figure(figsize=(7,7))
plt.pie(pred_counts, labels=pred_counts.index, autopct='%1.1f%%', startangle=140, colors=['#66b3ff','#ff9999','#99ff99'])
plt.title('Predicted Sentiment Distribution')
plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular
plt.show()

low_row = data.iloc[5115]
print(low_row)

import re

# 1. Extract the raw sentence at row 5115 (adjust 'SentenceSentiment0' if your column name differs)
sentence_2 = data.loc[2, 'Sentence']
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\$\w+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text
cleaned_sentence = clean_text(sentence_2)
sentence_vec = vectorizer.transform([cleaned_sentence])

# 5. Predict sentiment using your trained Logistic Regression model
predicted_sentiment = model.predict(sentence_vec)

print(f"Original sentence (row 2): {sentence_2}")
print(f"Predicted Sentiment: {predicted_sentiment[0]}")

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import (
    classification_report, accuracy_score,
    precision_recall_fscore_support, confusion_matrix,
    ConfusionMatrixDisplay
)

def conclude_sentiment_analysis(model, vectorizer, X_test, y_test, y_pred):
    # 1 classification report
    print("===== Classification Report =====")
    print(classification_report(y_test, y_pred))

    #  Overall accuracy and weighted metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Weighted Precision: {precision:.4f}")
    print(f"Weighted Recall: {recall:.4f}")
    print(f"Weighted F1-score: {f1:.4f}")

    #  confusion matrix
    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()

    #  predicted sentiment distribution pie chart
    pred_counts = pd.Series(y_pred).value_counts()
    plt.figure(figsize=(7,7))
    plt.pie(pred_counts, labels=pred_counts.index, autopct='%1.1f%%', startangle=140, colors=['#66b3ff','#ff9999','#99ff99'])
    plt.title('Predicted Sentiment Distribution')
    plt.axis('equal')
    plt.show()

    print("\nFinal Conclusion:")
    print(f"- The model achieves an overall accuracy of {accuracy:.2%}.")
    print(f"- It performs best on classes with high support and struggles more with minority classes, especially if imbalance exists.")
    print("- The confusion matrix and distribution plot highlight the majority of predictions are 'neutral', with fewer 'positive' and very few 'negative' predictions.")
    print("- Future work should focus on improving minority class performance through data balancing and advanced modeling, such as domain-specific Transformers (e.g., FinBERT).")

conclude_sentiment_analysis(model, vectorizer, X_test, y_test, y_pred)

import pandas as pd

# Create a DataFrame with actual vs predicted
results_df = pd.DataFrame({
    'Text': data.loc[y_test.index, 'cleaned_text'], # Get cleaned text using y_test indices
    'Actual_Sentiment': y_test,
    'Predicted_Sentiment': y_pred
})

# Save to CSV
results_df.to_csv('sentiment_results.csv', index=False)

print("Results saved to sentiment_results.csv")

import os
import pandas as pd

# Create DataFrame with actual vs predicted (your existing code)
results_df = pd.DataFrame({
    'Text': data.loc[y_test.index, 'cleaned_text'],  # Get cleaned text for test set
    'Actual_Sentiment': y_test,
    'Predicted_Sentiment': y_pred
})
file_path = 'sentiment_results.xlsx'
results_df.to_excel(file_path, index=False)
print(f"Results saved to {file_path}")

import pandas as pd
pred_counts = pd.Series(y_pred).value_counts()

#  percentage distribution of each class
pred_percentages = pred_counts / pred_counts.sum() * 100

# Combined counts and percentages into one DataFrame for display
summary_df = pd.DataFrame({
    'Count': pred_counts,
    'Percentage': pred_percentages
}).sort_index()
# Print overall predicted distribution
print("Overall Predicted Sentiment Distribution:")
print(summary_df)

# Optional: Format percentage to 2 decimal places
print("\nFormatted Percentage Distribution:")
print(summary_df.assign(Percentage=lambda x: x.Percentage.map('{:.2f}%'.format)))

import pandas as pd
import matplotlib.pyplot as plt
pred_counts = pd.Series(y_pred).value_counts().sort_index()

#  percentage distribution
pred_percentages = pred_counts / pred_counts.sum() * 100

#  line graph for counts and percentages
plt.figure(figsize=(10, 6))

#  counts line
plt.plot(pred_counts.index, pred_counts.values, marker='o', label='Count')

#  percentages line
plt.plot(pred_percentages.index, pred_percentages.values, marker='o', linestyle='--', label='Percentage (%)')

plt.title('Overall Predicted Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count / Percentage')
plt.grid(True)
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Pie chart
plt.figure(figsize=(6,6))
plt.pie(pred_counts, labels=pred_counts.index, autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff','#99ff99'])
plt.title('Predicted Sentiment Distribution')
plt.axis('equal')
plt.show()

# Bar chart
plt.figure(figsize=(8,5))
plt.bar(pred_counts.index, pred_counts.values, color=['#ff9999','#66b3ff','#99ff99'])
plt.title('Predicted Sentiment Counts')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

import os
import pandas as pd
# Detailed results dataframe
results_df = pd.DataFrame({
    'Text': data.loc[y_test.index, 'cleaned_text'],
    'Actual_Sentiment': y_test,
    'Predicted_Sentiment': y_pred
})

# Summary DataFrame with counts and percentages of predicted classes
pred_counts = pd.Series(y_pred).value_counts().sort_index()
pred_percentages = pred_counts / pred_counts.sum() * 100
summary_df = pd.DataFrame({
    'Count': pred_counts,
    'Percentage': pred_percentages
})
file_path = 'sentiment_analysis_results_for_tableau.xlsx'
with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
    results_df.to_excel(writer, sheet_name='Predictions', index=False)
    summary_df.to_excel(writer, sheet_name='Summary')

print(f"Sentiment analysis results saved for Tableau at: {file_path}")